{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import PySpark\n",
    "Nogle få kommentarer:\n",
    " - Download spark pre-built for hadoop 2.6, jeg vil også anbefale jer at bruge spark 1.6.0 da der er nogle problemer med 1.6.1 [hent den her http://www.apache.org/dyn/closer.lua/spark/spark-1.6.0/spark-1.6.0-bin-hadoop2.6.tgz]\n",
    " - husk at ændre paths i denne notebook\n",
    " - `os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--packages com.databricks:spark-avro_2.10:2.0.1 pyspark-shell\"` vil give jer mulighed for direkte at loade avro filer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "SPARK_HOME = \"\"\"spark-1.6.0-bin-hadoop2.6/\"\"\" ## PATH TO SPARK\n",
    "\n",
    "sys.path.append(os.path.join(SPARK_HOME, \"python\", \"lib\", \"py4j-0.9-src.zip\"))\n",
    "sys.path.append(os.path.join(SPARK_HOME, \"python\", \"lib\", \"pyspark.zip\"))\n",
    "os.environ[\"SPARK_HOME\"] = SPARK_HOME\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--packages com.databricks:spark-avro_2.10:2.0.1 pyspark-shell\"\n",
    "os.environ[\"PYSPARK_PYTHON\"]=\"/usr/bin/python3\"\n",
    "\n",
    "\n",
    "from pyspark import SparkConf, SparkContext, StorageLevel\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "conf = (SparkConf()\n",
    "         .setMaster(\"local[*]\")\n",
    "         .setAppName(\"My app\"))\n",
    "sc = SparkContext(conf = conf)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load data to dataframe\n",
    "df = (sqlContext.read.format(\"com.databricks.spark.avro\")\n",
    "      .load(\"data/201512/*.avro\")\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# I kan se strukturen på data her\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# kig på data\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transform til rdd\n",
    "data = df.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#more imports\n",
    "from DatabaseHelper import DatabaseHelper\n",
    "from dateutil import parser\n",
    "\n",
    "db = DatabaseHelper()\n",
    "\n",
    "first_period_min_date = parser.parse(\"2015-12-01 00:00:00+00:00\")\n",
    "first_period_max_date = parser.parse(\"2015-12-31 23:59:59+00:00\")\n",
    "second_period_min_date = parser.parse(\"2016-01-01 00:00:00+00:00\")\n",
    "second_period_max_date = parser.parse(\"2016-01-31 23:59:59+00:00\")\n",
    "third_period_min_date = parser.parse(\"2016-02-01 00:00:00+00:00\")\n",
    "third_period_max_date = parser.parse(\"2016-02-29 23:59:59+00:00\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#filtering\n",
    "swe_data = data.filter(lambda row: \"Sweden\" in row[\"country\"]).filter(lambda row: parser.parse(row[\"start_time\"]) >= first_period_min_date\n",
    "                           and parser.parse(row[\"end_time\"]) <= third_period_max_date)\n",
    "# conversion\n",
    "def convert_time_and_spatial(row):\n",
    "    return [((db.calculate_spatial_bin(row[\"longitude\"], row[\"latitude\"]),x),[row[\"useruuid\"]]) for x in db.calculate_time_bins(row[\"start_time\"], row[\"end_time\"])]\n",
    "\n",
    "swe_data = swe_data.flatMap(convert_time_and_spatial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "swe_data = swe_data.reduceByKey(lambda a, b: a+b if b[0] not in a else a) #key: (spatial_bin, time_bin), val: users\n",
    "swe_data.take(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
