{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import PySpark\n",
    "Nogle få kommentarer:\n",
    " - Download spark pre-built for hadoop 2.6, jeg vil også anbefale jer at bruge spark 1.6.0 da der er nogle problemer med 1.6.1 [hent den her http://www.apache.org/dyn/closer.lua/spark/spark-1.6.0/spark-1.6.0-bin-hadoop2.6.tgz]\n",
    " - husk at ændre paths i denne notebook\n",
    " - `os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--packages com.databricks:spark-avro_2.10:2.0.1 pyspark-shell\"` vil give jer mulighed for direkte at loade avro filer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "SPARK_HOME = \"\"\"spark-1.6.0-bin-hadoop2.6/\"\"\" ## PATH TO SPARK\n",
    "\n",
    "sys.path.append(os.path.join(SPARK_HOME, \"python\", \"lib\", \"py4j-0.9-src.zip\"))\n",
    "sys.path.append(os.path.join(SPARK_HOME, \"python\", \"lib\", \"pyspark.zip\"))\n",
    "os.environ[\"SPARK_HOME\"] = SPARK_HOME\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--packages com.databricks:spark-avro_2.10:2.0.1 pyspark-shell\"\n",
    "os.environ[\"PYSPARK_PYTHON\"]=\"/usr/bin/python3\"\n",
    "\n",
    "\n",
    "from pyspark import SparkConf, SparkContext, StorageLevel\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DecimalType, DoubleType, FloatType, ByteType, IntegerType, LongType, ArrayType\n",
    "\n",
    "conf = (SparkConf()\n",
    "         .setMaster(\"local[*]\")\n",
    "         .setAppName(\"My app\"))\n",
    "sc = SparkContext(conf = conf)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load data to dataframe\n",
    "df = (sqlContext.read.format(\"com.databricks.spark.avro\")\n",
    "      .load(\"data/201512/*.avro\")\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# I kan se strukturen på data her\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# kig på data\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dateutil import parser\n",
    "import math\n",
    "\n",
    "grid_boundaries_tuple=(-180, 180, -90, 90)\n",
    "spatial_resolution_decimals = 3\n",
    "\n",
    "GRID_MIN_LNG = (grid_boundaries_tuple[0] + 180) * pow(10,spatial_resolution_decimals)\n",
    "GRID_MAX_LNG = (grid_boundaries_tuple[1] + 180) * pow(10,spatial_resolution_decimals)\n",
    "GRID_MIN_LAT = (grid_boundaries_tuple[2] + 90) * pow(10,spatial_resolution_decimals)\n",
    "GRID_MAX_LAT = (grid_boundaries_tuple[3] + 90) * pow(10,spatial_resolution_decimals)\n",
    "\n",
    "def calculate_spatial_bin(lng, lat):\n",
    "    #lng = lst[0]\n",
    "    #lat = lst[1]\n",
    "    lat += 90.0\n",
    "    lng += 180.0\n",
    "    lat = math.trunc(lat*pow(10, spatial_resolution_decimals))\n",
    "    lng = math.trunc(lng*pow(10, spatial_resolution_decimals))\n",
    "    return (abs(GRID_MAX_LAT - GRID_MIN_LAT) *\n",
    "            (lat-GRID_MIN_LAT)) + (lng-GRID_MIN_LNG)\n",
    "\n",
    "def calculate_time_bins(start_time, end_time=None):\n",
    "    start_time = parser.parse(start_time)\n",
    "    min_datetime = parser.parse('2015-08-09 00:00:00+02')\n",
    "    start_bin = math.floor(\n",
    "        ((start_time-min_datetime).total_seconds()/60.0)/60)\n",
    "\n",
    "    if end_time:\n",
    "        end_time = parser.parse(end_time)\n",
    "        end_bin = math.ceil(((end_time-min_datetime).total_seconds()/60.0)/60)\n",
    "    else:\n",
    "        end_bin = start_bin\n",
    "\n",
    "    if start_bin == end_bin:\n",
    "        return [start_bin]\n",
    "    else:\n",
    "        return list(range(start_bin, end_bin))\n",
    "\n",
    "first_period_min_date = parser.parse(\"2015-12-01 00:00:00+00:00\")\n",
    "first_period_max_date = parser.parse(\"2015-12-09 23:59:59+00:00\")\n",
    "second_period_min_date = parser.parse(\"2015-12-10 00:00:00+00:00\")\n",
    "second_period_max_date = parser.parse(\"2015-12-19 23:59:59+00:00\")\n",
    "third_period_min_date = parser.parse(\"2015-12-20 00:00:00+00:00\")\n",
    "third_period_max_date = parser.parse(\"2015-12-29 23:59:59+00:00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#filter by country (Sweden) and start_time and end_time\n",
    "swe_data = data.filter(data[\"country\"] == 'Sweden').filter(data[\"start_time\"] >= first_period_min_date).filter(data[\"end_time\"] <= third_period_max_date)\n",
    "swe_data.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "swe_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#DecimalType, DoubleType, FloatType, ByteType, IntegerType, LongType\n",
    "udf_spatial_bin = udf(calculate_spatial_bin, IntegerType())\n",
    "binned_swe_data = swe_data.withColumn(\"spatial_bin\", udf_spatial_bin(\"longitude\", \"latitude\"))\n",
    "udf_time_bins = udf(calculate_time_bins, ArrayType(IntegerType()))\n",
    "binned_swe_data = binned_swe_data.withColumn(\"time_bins\", udf_time_bins(\"start_time\", \"end_time\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "binned_swe_data.select(binned_swe_data['start_time'], binned_swe_data['end_time'], binned_swe_data['time_bins']).sort('start_time').show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions\n",
    "# divide into periods\n",
    "first_period_min_bin = calculate_time_bins(\"2015-12-01 00:00:00+00:00\")[0]\n",
    "first_period_max_bin = calculate_time_bins(\"2015-12-09 23:59:59+00:00\")[0]\n",
    "second_period_min_bin = calculate_time_bins(\"2015-12-10 00:00:00+00:00\")[0]\n",
    "second_period_max_bin = calculate_time_bins(\"2015-12-19 23:59:59+00:00\")[0]\n",
    "third_period_min_bin = calculate_time_bins(\"2015-12-20 00:00:00+00:00\")[0]\n",
    "third_period_max_bin = calculate_time_bins(\"2015-12-31 23:59:59+00:00\")[0]\n",
    "\n",
    "\n",
    "period_1_locations = binned_swe_data.filter(binned_swe_data[\"start_time\"] >= first_period_min_date).filter(binned_swe_data[\"end_time\"] < first_period_max_date)\n",
    "period_2_locations = binned_swe_data.filter(binned_swe_data[\"start_time\"] >= second_period_min_date).filter(binned_swe_data[\"end_time\"] < second_period_max_date)\n",
    "period_3_locations = binned_swe_data.filter(binned_swe_data[\"start_time\"] >= third_period_min_date).filter(binned_swe_data[\"end_time\"] < third_period_max_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove duplicates and create (users) -> [(spatial_bin, time_bin)]\n",
    "distinct = period_1_locations.select(period_1_locations.useruuid, period_1_locations.spatial_bin, pyspark.sql.functions.explode(period_1_locations.time_bins)).distinct()\n",
    "distinct.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate cooccurrences in form of: (user1,user2) -> [(spatial,time)]\n",
    "from itertools import combinations\n",
    "def generate_cooccurrences(row):\n",
    "    return [(tuple(sorted(pair)),[row[0]]) for pair in combinations(row[1], 2)]\n",
    "    \n",
    "coocs_1 = period_1_bins_to_users.flatMap(generate_cooccurrences).reduceByKey(lambda a,b: a+b)\n",
    "coocs_2 = period_2_bins_to_users.flatMap(generate_cooccurrences).reduceByKey(lambda a,b: a+b)\n",
    "coocs_3 = period_3_bins_to_users.flatMap(generate_cooccurrences).reduceByKey(lambda a,b: a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coocs_1.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# generate location entropies (H_l) for use in weighted frequency in form key:spatial_bin, val: H_l\n",
    "def calculate_H(row):\n",
    "    H_val = 0\n",
    "    for user in set(row[1]):\n",
    "        P_ul = row[1].count(user)/len(row[1])\n",
    "        H_val += P_ul*np.log2(P_ul)\n",
    "    return row[0],-H_val\n",
    "\n",
    "period_1_h_vals = period_1_bins_to_users.map(lambda row: (row[0][0],row[1])).reduceByKey(lambda a, b: a+b).map(calculate_H).collectAsMap()\n",
    "period_2_h_vals = period_2_bins_to_users.map(lambda row: (row[0][0],row[1])).reduceByKey(lambda a, b: a+b).map(calculate_H).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "y_1_users = coocs_2.map(lambda row: row[0]).collect()\n",
    "y_2_users = coocs_3.map(lambda row: row[0]).collect()\n",
    "\n",
    "def compute_weighted_frequency(row, h_vals):\n",
    "    spatial_bins = [r[0] for r in row[1]]\n",
    "    wf_value = 0\n",
    "    for sb in set(spatial_bins):\n",
    "        wf_value += spatial_bins.count(sb)*np.exp(-h_vals[sb])\n",
    "    return wf_value\n",
    "\n",
    "def compute_features(y, row, h_vals):\n",
    "    # number of cooccurrences\n",
    "    num_coocs = len(row[1])\n",
    "    # number of unique (by spatial bin) cooccurrences\n",
    "    num_unique_coocs = len(set([r[0] for r in row[1]]))\n",
    "    spatial_bins = [r[0] for r in row[1]]\n",
    "    # weighted frequency\n",
    "    weighted_frequency = compute_weighted_frequency(row, h_vals)\n",
    "    # diversity\n",
    "    diversity = -np.sum([spatial_bins.count(sb)/len(spatial_bins)*np.log2(spatial_bins.count(sb)/len(spatial_bins)) for sb in set(spatial_bins)])\n",
    "    return LabeledPoint(y, [num_coocs, num_unique_coocs, diversity, weighted_frequency])\n",
    "\n",
    "def compute_train_features(row):\n",
    "    y = 1 if row[0] in y_1_users else 0\n",
    "    return compute_features(y, row, period_1_h_vals)\n",
    "def compute_test_features(row):\n",
    "    y = 1 if row[0] in y_2_users else 0\n",
    "    return compute_features(y, row, period_2_h_vals)\n",
    "\n",
    "X_train = coocs_1.map(compute_train_features)\n",
    "\n",
    "X_test = coocs_2.map(compute_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "# Train model and compute AUC\n",
    "model = RandomForest.trainClassifier(X_train, numClasses=2, categoricalFeaturesInfo={},\n",
    "                                     numTrees=100, featureSubsetStrategy=\"auto\",\n",
    "                                     impurity='gini', maxDepth=4, maxBins=32)\n",
    "\n",
    "\n",
    "\n",
    "predictions = model.predict(X_test.map(lambda x: x.features))\n",
    "#print(predictions.count())\n",
    "\n",
    "labels = X_test.map(lambda x: x.label)\n",
    "#print(labels.count())\n",
    "\n",
    "predictionAndLabels = predictions.zip(labels)\n",
    "#print(predictionAndLabels.count())\n",
    "\n",
    "metrics = BinaryClassificationMetrics(predictionAndLabels)\n",
    "print(\"Area under ROC = {}\".format(metrics.areaUnderROC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
