{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import PySpark\n",
    "Nogle få kommentarer:\n",
    " - Download spark pre-built for hadoop 2.6, jeg vil også anbefale jer at bruge spark 1.6.0 da der er nogle problemer med 1.6.1 [hent den her http://www.apache.org/dyn/closer.lua/spark/spark-1.6.0/spark-1.6.0-bin-hadoop2.6.tgz]\n",
    " - husk at ændre paths i denne notebook\n",
    " - `os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--packages com.databricks:spark-avro_2.10:2.0.1 pyspark-shell\"` vil give jer mulighed for direkte at loade avro filer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "SPARK_HOME = \"\"\"spark-1.6.0-bin-hadoop2.6/\"\"\" ## PATH TO SPARK\n",
    "\n",
    "sys.path.append(os.path.join(SPARK_HOME, \"python\", \"lib\", \"py4j-0.9-src.zip\"))\n",
    "sys.path.append(os.path.join(SPARK_HOME, \"python\", \"lib\", \"pyspark.zip\"))\n",
    "os.environ[\"SPARK_HOME\"] = SPARK_HOME\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--packages com.databricks:spark-avro_2.10:2.0.1 pyspark-shell\"\n",
    "os.environ[\"PYSPARK_PYTHON\"]=\"/usr/bin/python3\"\n",
    "\n",
    "\n",
    "from pyspark import SparkConf, SparkContext, StorageLevel\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "conf = (SparkConf()\n",
    "         .setMaster(\"local[*]\")\n",
    "         .setAppName(\"My app\"))\n",
    "sc = SparkContext(conf = conf)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load data to dataframe\n",
    "df = (sqlContext.read.format(\"com.databricks.spark.avro\")\n",
    "      .load(\"data/201512/*.avro\")\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# I kan se strukturen på data her\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# kig på data\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transform til rdd\n",
    "data = df.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#more imports\n",
    "from DatabaseHelper import DatabaseHelper\n",
    "from dateutil import parser\n",
    "\n",
    "db = DatabaseHelper()\n",
    "\n",
    "first_period_min_date = parser.parse(\"2015-12-01 00:00:00+00:00\")\n",
    "first_period_max_date = parser.parse(\"2015-12-31 23:59:59+00:00\")\n",
    "second_period_min_date = parser.parse(\"2016-01-01 00:00:00+00:00\")\n",
    "second_period_max_date = parser.parse(\"2016-01-31 23:59:59+00:00\")\n",
    "third_period_min_date = parser.parse(\"2016-02-01 00:00:00+00:00\")\n",
    "third_period_max_date = parser.parse(\"2016-02-29 23:59:59+00:00\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#filtering\n",
    "swe_data = data.filter(lambda row: \"Sweden\" in row[\"country\"]).filter(lambda row: parser.parse(row[\"start_time\"]) >= first_period_min_date\n",
    "                           and parser.parse(row[\"end_time\"]) <= third_period_max_date)\n",
    "\n",
    "# convert to ((spatial, time)[useruuid]) rows\n",
    "def convert_time_and_spatial(row):\n",
    "    return [((db.calculate_spatial_bin(row[\"longitude\"], row[\"latitude\"]),x),[row[\"useruuid\"]]) for x in db.calculate_time_bins(row[\"start_time\"], row[\"end_time\"])]\n",
    "\n",
    "swe_data = swe_data.flatMap(convert_time_and_spatial)\n",
    "# remove duplicates, key: (spatial_bin, time_bin), val: users\n",
    "swe_data = swe_data.reduceByKey(lambda a, b: a+b if b[0] not in a else a)\n",
    "swe_data.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# divide into periods\n",
    "first_period_min_bin = db.calculate_time_bins(\"2015-12-01 00:00:00+00:00\")[0]\n",
    "first_period_max_bin = db.calculate_time_bins(\"2015-12-09 23:59:59+00:00\")[0]\n",
    "second_period_min_bin = db.calculate_time_bins(\"2015-12-10 00:00:00+00:00\")[0]\n",
    "second_period_max_bin = db.calculate_time_bins(\"2015-12-19 23:59:59+00:00\")[0]\n",
    "third_period_min_bin = db.calculate_time_bins(\"2015-12-20 00:00:00+00:00\")[0]\n",
    "third_period_max_bin = db.calculate_time_bins(\"2015-12-31 23:59:59+00:00\")[0]\n",
    "\n",
    "period_1_data = swe_data.filter(lambda row: row[0][1] >= first_period_min_bin\n",
    "                                and row[0][1] < first_period_max_bin)\n",
    "period_2_data = swe_data.filter(lambda row: row[0][1] >= second_period_min_bin\n",
    "                                and row[0][1] < second_period_max_bin)\n",
    "period_3_data = swe_data.filter(lambda row: row[0][1] >= third_period_min_bin\n",
    "                                and row[0][1] < third_period_max_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(period_1_data.count())\n",
    "print(period_2_data.count())\n",
    "print(period_3_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "def generate_cooccurrences(row):\n",
    "    return [(tuple(sorted(pair)),[row[0]]) for pair in combinations(row[1], 2)]\n",
    "    \n",
    "coocs_1 = period_1_data.flatMap(generate_cooccurrences).reduceByKey(lambda a,b: a+b)\n",
    "coocs_2 = period_2_data.flatMap(generate_cooccurrences).reduceByKey(lambda a,b: a+b)\n",
    "coocs_3 = period_3_data.flatMap(generate_cooccurrences).reduceByKey(lambda a,b: a+b)\n",
    "                                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "\n",
    "y_1_users = coocs_2.map(lambda row: row[0]).collect()\n",
    "\n",
    "y_2_users = coocs_3.map(lambda row: row[0]).collect()\n",
    "\n",
    "X_train_num_coocs = coocs_1.map(lambda row: LabeledPoint(1 if row[0] in y_1_users else 0,[len(row[1])]))\n",
    "\n",
    "X_test_num_coocs = coocs_2.map(lambda row: LabeledPoint(1 if row[0] in y_2_users else 0,[len(row[1])]))\n",
    "# more features can be computed with join on different features maybe?\n",
    "#X.take(1)\n",
    "#X_train = RowMatrix(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "model = RandomForest.trainClassifier(X, numClasses=2, categoricalFeaturesInfo={},\n",
    "                                     numTrees=3, featureSubsetStrategy=\"auto\",\n",
    "                                     impurity='gini', maxDepth=4, maxBins=32)\n",
    "\n",
    "predictions = model.predict(X_test_num_coocs)\n",
    "\n",
    "predictions = model.predict(X_test_num_coocs.map(lambda x: x.features))\n",
    "labelsAndPredictions = X_test_num_coocs.map(lambda lp: lp.label).zip(predictions)\n",
    "testMSE = labelsAndPredictions.map(lambda vp: (vp[0] - vp[1]) * (vp[0] - vp[1])).sum() /\\\n",
    "    float(X_test_num_coocs.count())\n",
    "print(\"Test Mean Squared Error = {}\".format(str(testMSE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
