{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import PySpark\n",
    "Nogle få kommentarer:\n",
    " - Download spark pre-built for hadoop 2.6, jeg vil også anbefale jer at bruge spark 1.6.0 da der er nogle problemer med 1.6.1 [hent den her http://www.apache.org/dyn/closer.lua/spark/spark-1.6.0/spark-1.6.0-bin-hadoop2.6.tgz]\n",
    " - husk at ændre paths i denne notebook\n",
    " - `os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--packages com.databricks:spark-avro_2.10:2.0.1 pyspark-shell\"` vil give jer mulighed for direkte at loade avro filer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run when local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "SPARK_HOME = \"\"\"spark-1.6.0-bin-hadoop2.6/\"\"\" ## PATH TO SPARK\n",
    "\n",
    "sys.path.append(os.path.join(SPARK_HOME, \"python\", \"lib\", \"py4j-0.9-src.zip\"))\n",
    "sys.path.append(os.path.join(SPARK_HOME, \"python\", \"lib\", \"pyspark.zip\"))\n",
    "os.environ[\"SPARK_HOME\"] = SPARK_HOME\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--packages com.databricks:spark-avro_2.10:2.0.1 pyspark-shell\"\n",
    "os.environ[\"PYSPARK_PYTHON\"]=\"/usr/bin/python3\"\n",
    "\n",
    "\n",
    "from pyspark import SparkConf, SparkContext, StorageLevel\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DecimalType, DoubleType, FloatType, ByteType, IntegerType, LongType, ArrayType, StringType\n",
    "\n",
    "conf = (SparkConf()\n",
    "         .setMaster(\"local[*]\")\n",
    "         .setAppName(\"My app\"))\n",
    "sc = SparkContext(conf = conf)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load data to dataframe\n",
    "data = (sqlContext.read.format(\"com.databricks.spark.avro\")\n",
    "      .load(\"data/201509/*.avro\")\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run when on cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load data to dataframe\n",
    "data = (sqlContext.read.format(\"com.databricks.spark.avro\")\n",
    "      .load(\"s3://sbdp-source-lifelog/environments/prod/revisions/1/location/yearmonth=201509/*.avro\")\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# I kan se strukturen på data her\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# kig på data\n",
    "data.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define bin calculation functions and periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext, StorageLevel\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DecimalType, DoubleType, FloatType, ByteType, IntegerType, LongType, ArrayType, StringType\n",
    "from dateutil import parser\n",
    "import math\n",
    "\n",
    "grid_boundaries_tuple=(-180, 180, -90, 90)\n",
    "spatial_resolution_decimals = 3\n",
    "\n",
    "GRID_MIN_LNG = (grid_boundaries_tuple[0] + 180) * pow(10,spatial_resolution_decimals)\n",
    "GRID_MAX_LNG = (grid_boundaries_tuple[1] + 180) * pow(10,spatial_resolution_decimals)\n",
    "GRID_MIN_LAT = (grid_boundaries_tuple[2] + 90) * pow(10,spatial_resolution_decimals)\n",
    "GRID_MAX_LAT = (grid_boundaries_tuple[3] + 90) * pow(10,spatial_resolution_decimals)\n",
    "\n",
    "def calculate_spatial_bin(lng, lat):\n",
    "    lat += 90.0\n",
    "    lng += 180.0\n",
    "    lat = math.trunc(lat*pow(10, spatial_resolution_decimals))\n",
    "    lng = math.trunc(lng*pow(10, spatial_resolution_decimals))\n",
    "    return (abs(GRID_MAX_LAT - GRID_MIN_LAT) *\n",
    "            (lat-GRID_MIN_LAT)) + (lng-GRID_MIN_LNG)\n",
    "\n",
    "def calculate_time_bins(start_time, end_time=None):\n",
    "    start_time = parser.parse(start_time)\n",
    "    min_datetime = parser.parse('2015-08-09 00:00:00+02')\n",
    "    start_bin = int(math.floor(\n",
    "        ((start_time-min_datetime).total_seconds()/60.0)/60))\n",
    "\n",
    "    if end_time:\n",
    "        end_time = parser.parse(end_time)\n",
    "        end_bin = int(math.ceil(((end_time-min_datetime).total_seconds()/60.0)/60))\n",
    "    else:\n",
    "        end_bin = start_bin\n",
    "\n",
    "    if start_bin == end_bin:\n",
    "        return [start_bin]\n",
    "    else:\n",
    "        return list(range(start_bin, end_bin))\n",
    "\n",
    "# string reps of periods\n",
    "first_period_min = \"2015-09-01 00:00:00+00:00\"\n",
    "first_period_max = \"2015-09-09 23:59:59+00:00\"\n",
    "second_period_min = \"2015-09-10 00:00:00+00:00\"\n",
    "second_period_max = \"2015-09-19 23:59:59+00:00\"\n",
    "third_period_min = \"2015-09-20 00:00:00+00:00\"\n",
    "third_period_max = \"2015-09-30 23:59:59+00:00\"\n",
    "\n",
    "# datetime objects of periods\n",
    "first_period_min_date = parser.parse(first_period_min)\n",
    "first_period_max_date = parser.parse(first_period_max)\n",
    "second_period_min_date = parser.parse(second_period_min)\n",
    "second_period_max_date = parser.parse(second_period_max)\n",
    "third_period_min_date = parser.parse(third_period_min)\n",
    "third_period_max_date = parser.parse(third_period_max)\n",
    "\n",
    "# timebins of periods\n",
    "first_period_min_bin = calculate_time_bins(first_period_min)[0]\n",
    "first_period_max_bin = calculate_time_bins(first_period_max)[0]\n",
    "second_period_min_bin = calculate_time_bins(second_period_min)[0]\n",
    "second_period_max_bin = calculate_time_bins(second_period_max)[0]\n",
    "third_period_min_bin = calculate_time_bins(third_period_min)[0]\n",
    "third_period_max_bin = calculate_time_bins(third_period_max)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframes: filter by country, spatial_bin, time_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, explode\n",
    "from pyspark.sql.types import DecimalType, DoubleType, FloatType, ByteType, IntegerType, LongType, ArrayType\n",
    "\n",
    "#filter by country (Sweden) and start_time and end_time\n",
    "swe_data = data.filter(data[\"country\"] == 'Sweden').filter(data[\"start_time\"] >= first_period_min_date).filter(data[\"end_time\"] <= third_period_max_date)\n",
    "\n",
    "udf_spatial_bin = udf(calculate_spatial_bin, IntegerType())\n",
    "# add new column spatial_bin\n",
    "binned_swe_data = swe_data.withColumn(\"spatial_bin\", udf_spatial_bin(\"longitude\", \"latitude\"))\n",
    "# add new column time_bins\n",
    "udf_time_bins = udf(calculate_time_bins, ArrayType(IntegerType()))\n",
    "binned_swe_data = binned_swe_data.withColumn(\"time_bins\", udf_time_bins(\"start_time\", \"end_time\"))\n",
    "# get distinct spatial bin, time bin, useruuid rows only\n",
    "binned_swe_data = binned_swe_data.select(binned_swe_data[\"spatial_bin\"], explode(binned_swe_data[\"time_bins\"]).alias(\"time_bin\"), binned_swe_data[\"useruuid\"]).distinct()\n",
    "\n",
    "# reduce to (spatial_bin, time_bin) -> [users] (CONVERSION TO RDD)\n",
    "bins_to_users = binned_swe_data.rdd.map(lambda r: ((r[0],r[1]),[r[2]])).reduceByKey(lambda a, b: a+b if b[0] not in a else a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define criteria for users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "timebin_percentage = 0.4\n",
    "period_1_max_timebins = first_period_max_bin - first_period_min_bin\n",
    "period_2_max_timebins = second_period_max_bin - second_period_min_bin\n",
    "period_3_max_timebins = third_period_max_bin - third_period_min_bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find users for each period (RDD version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "period_1_users = binned_swe_data.filter(binned_swe_data[\"time_bin\"] < first_period_max_bin).filter(binned_swe_data[\"time_bin\"] >= first_period_min_bin).select(binned_swe_data[\"useruuid\"]).map(lambda r: ((r),1)).reduceByKey(lambda a, b: a+b).filter(lambda r: r[1] / float(period_1_max_timebins) > timebin_percentage).map(lambda r: r[0])\n",
    "period_2_users = binned_swe_data.filter(binned_swe_data[\"time_bin\"] < second_period_max_bin).filter(binned_swe_data[\"time_bin\"] >= second_period_min_bin).select(binned_swe_data[\"useruuid\"]).map(lambda r: ((r),1)).reduceByKey(lambda a, b: a+b).filter(lambda r: r[1] / float(period_2_max_timebins) > timebin_percentage).map(lambda r: r[0])\n",
    "period_3_users = binned_swe_data.filter(binned_swe_data[\"time_bin\"] < third_period_max_bin).filter(binned_swe_data[\"time_bin\"] >= third_period_min_bin).select(binned_swe_data[\"useruuid\"]).map(lambda r: ((r),1)).reduceByKey(lambda a, b: a+b).filter(lambda r: r[1] / float(period_3_max_timebins) > timebin_percentage).map(lambda r: r[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find users for each period (Dataframe version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "period_1_users = binned_swe_data.filter(binned_swe_data[\"time_bin\"] < first_period_max_bin).filter(binned_swe_data[\"time_bin\"] >= first_period_min_bin).select(binned_swe_data[\"useruuid\"]).groupBy(binned_swe_data[\"useruuid\"]).count()\n",
    "period_1_users = period_1_users.filter(period_1_users[\"count\"]/float(period_1_max_timebins) > timebin_percentage).select(binned_swe_data[\"useruuid\"])\n",
    "\n",
    "period_2_users = binned_swe_data.filter(binned_swe_data[\"time_bin\"] < second_period_max_bin).filter(binned_swe_data[\"time_bin\"] >= second_period_min_bin).select(binned_swe_data[\"useruuid\"]).groupBy(binned_swe_data[\"useruuid\"]).count()\n",
    "period_2_users = period_2_users.filter(period_2_users[\"count\"]/float(period_2_max_timebins) > timebin_percentage).select(binned_swe_data[\"useruuid\"])\n",
    "\n",
    "period_3_users = binned_swe_data.filter(binned_swe_data[\"time_bin\"] < third_period_max_bin).filter(binned_swe_data[\"time_bin\"] >= third_period_min_bin).select(binned_swe_data[\"useruuid\"]).groupBy(binned_swe_data[\"useruuid\"]).count()\n",
    "period_3_users = period_3_users.filter(period_3_users[\"count\"]/float(period_3_max_timebins) > timebin_percentage).select(binned_swe_data[\"useruuid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(period_1_users.count())\n",
    "print(period_2_users.count())\n",
    "print(period_3_users.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find users which are common to every period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "users_in_all = period_1_users.intersect(period_2_users).intersect(period_3_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide into periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "period_1_bins_to_users = bins_to_users.filter(lambda row: row[0][1] >= first_period_min_bin\n",
    "                                and row[0][1] < first_period_max_bin)\n",
    "period_2_bins_to_users = bins_to_users.filter(lambda row: row[0][1] >= second_period_min_bin\n",
    "                                and row[0][1] < second_period_max_bin)\n",
    "period_3_bins_to_users = bins_to_users.filter(lambda row: row[0][1] >= third_period_min_bin\n",
    "                                and row[0][1] < third_period_max_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Generate co-occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate cooccurrences in form of: (user1,user2) -> [(spatial,time)]\n",
    "from itertools import combinations\n",
    "def generate_cooccurrences(row):\n",
    "    return [(tuple(sorted(pair)),[row[0]]) for pair in combinations(row[1], 2)]\n",
    "    \n",
    "coocs_1 = period_1_bins_to_users.flatMap(generate_cooccurrences).reduceByKey(lambda a,b: a+b)\n",
    "coocs_2 = period_2_bins_to_users.flatMap(generate_cooccurrences).reduceByKey(lambda a,b: a+b)\n",
    "coocs_3 = period_3_bins_to_users.flatMap(generate_cooccurrences).reduceByKey(lambda a,b: a+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate location entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# generate location entropies (H_l) for use in weighted frequency in form key:spatial_bin, val: H_l\n",
    "def calculate_H(row):\n",
    "    H_val = 0\n",
    "    for user in set(row[1]):\n",
    "        P_ul = row[1].count(user)/float(len(row[1]))\n",
    "        H_val += P_ul*np.log2(P_ul)\n",
    "    return row[0],-H_val\n",
    "\n",
    "period_1_h_vals = sc.broadcast(period_1_bins_to_users.map(lambda row: (row[0][0],row[1])).reduceByKey(lambda a, b: a+b).map(calculate_H).collectAsMap())\n",
    "period_2_h_vals = sc.broadcast(period_2_bins_to_users.map(lambda row: (row[0][0],row[1])).reduceByKey(lambda a, b: a+b).map(calculate_H).collectAsMap())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "y_1_users = sc.broadcast(coocs_2.map(lambda row: row[0]).collect())\n",
    "y_2_users = sc.broadcast(coocs_3.map(lambda row: row[0]).collect())\n",
    "\n",
    "def compute_weighted_frequency(row, h_vals):\n",
    "    spatial_bins = [r[0] for r in row[1]]\n",
    "    wf_value = 0\n",
    "    for sb in set(spatial_bins):\n",
    "        wf_value += spatial_bins.count(sb)*np.exp(-h_vals.value[sb])\n",
    "    return wf_value\n",
    "\n",
    "def compute_features(y, row, h_vals):\n",
    "    # number of cooccurrences\n",
    "    num_coocs = len(row[1])\n",
    "    # number of unique (by spatial bin) cooccurrences\n",
    "    num_unique_coocs = len(set([r[0] for r in row[1]]))\n",
    "    spatial_bins = [r[0] for r in row[1]]\n",
    "    # weighted frequency\n",
    "    weighted_frequency = compute_weighted_frequency(row, h_vals)\n",
    "    # diversity\n",
    "    diversity = np.exp(-np.sum([spatial_bins.count(sb)/float(len(spatial_bins))*np.log2(spatial_bins.count(sb)/float(len(spatial_bins))) for sb in set(spatial_bins)]))\n",
    "    return LabeledPoint(y, [num_coocs, num_unique_coocs, diversity, weighted_frequency])\n",
    "\n",
    "def compute_train_features(row):\n",
    "    y = 1 if row[0] in y_1_users.value else 0\n",
    "    return compute_features(y, row, period_1_h_vals)\n",
    "def compute_test_features(row):\n",
    "    y = 1 if row[0] in y_2_users.value else 0\n",
    "    return compute_features(y, row, period_2_h_vals)\n",
    "\n",
    "X_train = coocs_1.map(compute_train_features)\n",
    "\n",
    "X_test = coocs_2.map(compute_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print X_train.filter(lambda row: row.label == 0).count()\n",
    "print X_train.filter(lambda row: row.label == 1).count()\n",
    "print X_test.filter(lambda row: row.label == 0).count()\n",
    "print X_test.filter(lambda row: row.label == 1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "# Train model and compute AUC\n",
    "model = RandomForest.trainClassifier(X_train, numClasses=2, categoricalFeaturesInfo={},\n",
    "                                     numTrees=100, featureSubsetStrategy=\"auto\",\n",
    "                                     impurity='gini', maxDepth=4, maxBins=32)\n",
    "\n",
    "\n",
    "\n",
    "predictions = model.predict(X_test.map(lambda x: x.features))\n",
    "#print(predictions.count())\n",
    "\n",
    "labels = X_test.map(lambda x: x.label)\n",
    "#print(labels.count())\n",
    "\n",
    "predictionAndLabels = predictions.zip(labels)\n",
    "#print(predictionAndLabels.count())\n",
    "\n",
    "metrics = BinaryClassificationMetrics(predictionAndLabels)\n",
    "print(\"Area under ROC = {}\".format(metrics.areaUnderROC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
